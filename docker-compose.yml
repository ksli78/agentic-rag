services:
  ollama:
    image: ollama/ollama:latest
    restart: unless-stopped
    ports:
      - "11434:11434"
    # If you want GPU for Ollama too, enable one of the following:
    # gpus: all       # for Docker Desktop / compose v2
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]
    volumes:
      - ollama:/root/.ollama
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 10s
      timeout: 3s
      retries: 20
      start_period: 10s

  api:
    build:
      context: ./fastapi
      # dockerfile: Dockerfile  # optional if your file is named Dockerfile
    depends_on:
      ollama:
        condition: service_healthy
    # Enable GPU for the API service
    # gpus: all       # for Docker Desktop / compose v2
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]
    environment:
      - OLLAMA_HOST=http://ollama:11434
      - OLLAMA_EMBED_MODEL=sentence-transformers/all-mpnet-base-v2  # <= update this line
      - OLLAMA_GEN_MODEL=llama3.2:latest
      - LANCEDB_URI=/data/lancedb
      - FAISS_INDEX_PATH=/data/faiss_index
      - CHUNK_SIZE=1200
      - CHUNK_OVERLAP=200
      # Optional Marker+Ollama settings
      - USE_MARKER=true
      - MARKER_USE_LLM=true
      - MARKER_OLLAMA_MODEL=llama3:8b
      - MARKER_DEVICE=cuda
    volumes:
      - lancedb:/data/lancedb
      - faiss_index:/data/faiss_index   # <- new volume for FAISS
    ports:
      - "8000:8000"

volumes:
  ollama:
  lancedb:
